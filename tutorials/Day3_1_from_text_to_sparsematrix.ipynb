{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn (sklearn)은 다양한 머신러닝 알고리즘 및 데이터 처리 모듈들을 가지고 있는 툴킷입니다. sklearn의 CountVectorizer를 이용하여 term frequency matrix를 만드는 법을 연습합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 앞선 연습에서 만들어둔 corpus를 읽어와 koNLPy의 Twitter 형태소 분석기를 이용하여 명사를 추출하는 custom tokenizer를 만들겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_data_fname = './tmp/reviews.txt'\n",
    "model_folder = './tmp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27689, 27689, 27689)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = []\n",
    "scores = []\n",
    "texts = []\n",
    "with open(preprocessed_data_fname, encoding='utf-8') as f:\n",
    "    for doc in f:\n",
    "        try:\n",
    "            idx, score, text = doc.split('\\t')\n",
    "            score = int(score)\n",
    "            idxs.append(idx)\n",
    "            scores.append(score)\n",
    "            texts.append(text)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "len(idxs), len(scores), len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python에서는 두 개의 for loop을 한 줄에 표현하는 list comprehension 표현이 있습니다. 이에 익숙해지면 Python 코드를 명료하고 간결하게 작성할 수 있습니다. \n",
    "\n",
    "list l1은 [ ['a', 'b'], [1, 2, 3] ] 처럼 두 개의 list를 가지고 있습니다. 이 두 개의 리스트의 모든 값을 일렬로 나열한 하나의 flatten list를 만들어봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'b'], [1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "l1 = [ ['a', 'b'], [1, 2, 3] ]\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 구문은 이렇게 해석할 수 있습니다. \n",
    "\n",
    "1. element 들을 하나루 묶은 list를 생성할 겁니다. [element]\n",
    "1. 그런데 이 element는 [1, 2, 3]과 같은 small_list 안의 원소를 의미합니다. 이는 다음과 같이 정의합니다. \n",
    "\n",
    "    for element in small_list\n",
    "    \n",
    "1. 그러나 small_list는 아직 정의된 적이 없습니다. element가 정의되기 전에 small_list가 정의되어야 합니다. 그렇기 때문에 for element in small_list 앞에서 small_list를 먼저 정의합니다. \n",
    "\n",
    "    for small_list in l1 for element in small_list\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "flatten_l1 = [element for small_list in l1 for element in small_list]\n",
    "print(flatten_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드는 아래와 동일합니다. 네 줄로 적어야 하는 부분이 한 줄로 표현됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "flatten_l1 = []\n",
    "for small_list in l1:\n",
    "    for element in small_list:\n",
    "        flatten_l1.append(element)\n",
    "print(flatten_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From corpus to noun term frequency sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter.nouns(sent) 는 주어진 sent에서 명사를 추출하여 return 합니다. type은 list of str입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이건', '테스트']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "twitter.nouns('이건 테스트입니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a line a document이기 때문에 docs[i]은 하나의 문서를 의미하며, 하나의 문서는 두 칸 띄어쓰기로 줄이 나뉘어져 있습니다. 그러나 우리는 지금 모든 문서에서 명사를 추출한 뒤, 각 명사들이 몇 번 나왔었는지를 알아보려 합니다. 그러므로 한 줄 한 줄을 굳이 나누지는 않겠습니다. \n",
    "\n",
    "앞서 배운 collections.Counter를 이용하여 한 번에 명사의 개수를 세어 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of nouns: 11758\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "noun_counter = Counter([noun for text in texts for noun in twitter.nouns(text)])\n",
    "print('num of nouns: %d' % len(noun_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "term frequency matrix를 만들 때, 거의 등장하지 않는 단어들은 의미가 없습니다. min count를 설정하여 각 threshold 별로 몇 개의 명사가 살아남는지 알아봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of nouns (min_count = 2): 5847\n",
      "num of nouns (min_count = 3): 4182\n",
      "num of nouns (min_count = 5): 2879\n",
      "num of nouns (min_count = 10): 1715\n"
     ]
    }
   ],
   "source": [
    "for min_count in [2, 3, 5, 10]:\n",
    "    _counter = {word for word, freq in noun_counter.items() if freq >= min_count}\n",
    "    print('num of nouns (min_count = %d): %d' % (min_count, len(_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 5번 이상 나오는 4102개의 명사를 이용하여 term frequency matrix를 만들어보겠습니다. CountVectorizer는 문서가 주어지면 띄어쓰기를 tokenizer로 이용합니다. filtering이 되지 않습니다. 이를 방지하기 위하여 빈도수가 5 이상인 명사만을 return 하는 custom tokenizer를 만들어 봅시다. \n",
    "\n",
    "Twitter.nouns()와 custom_tokenizer의 결과가 달라짐을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['제목', '덕필']\n",
      "['제목']\n"
     ]
    }
   ],
   "source": [
    "noun_dict = {word for word, freq in noun_counter.items() if freq >= 5}\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    return [word for word in twitter.nouns(doc) if word in noun_dict]\n",
    "\n",
    "print(twitter.nouns(texts[2]))\n",
    "print(custom_tokenizer(texts[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2879"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noun_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer는 document frequency를 기준으로 대부분 문서에 등장하거나 거의 등장하지 않는 극단적인 단어들을 제거할 수 있습니다. min_df와 max_df를 이용하면 되며, df는 비율입니다. [0, 1] 사이의 값을 입력해야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=custom_tokenizer, min_df=0.001, max_df=0.95)\n",
    "x_sparse = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_sparse는 scipy의 sparse matrix입니다. Sparse matrix는 0 값이 많은 행렬에 대하여 0이 아닌 (i, j)의 값만을 저장한 matrix를 의미합니다. Term frequency matrix는 very sparse matrix에 속하므로 dense matrix보다 sparse matrix가 훨씬 효율적입니다. dense matrix는 문서의 양이 조금만 커도 메모리가 폭발합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27689x785 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 104396 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer에서 각 column에 해당하는 term 알아내기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizer.vocabulary_ 는 dict 형식으로, {word:index} 입니다. \n",
    "\n",
    "이를 이용하여 vocab to index, index to vocab을 만들어 봅니다. 정부라는 단어가 1663에 해당합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2int = vectorizer.vocabulary_\n",
    "vocab2int['영화']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index는 0부터 시작합니다. 그러므로 int2vocab은 dictionary 형태보다 list 형태로 가지고 있어도 좋습니다. 이를 위해 sorted 함수를 이용합니다. 1663 번째 단어가 정부임을 다시 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'끝'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2vocab = [word for word,index in sorted(vocab2int.items(), key=lambda x:x[1])]\n",
    "int2vocab[97]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse matrix I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse matrix를 파일로 저장하고 부르는 것은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27689x785 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 104396 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import mmwrite, mmread\n",
    "\n",
    "mmwrite('./tmp/x.mm', x_sparse)\n",
    "loaded_x_sparse = mmread('./tmp/x.mm')\n",
    "loaded_x_sparse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
