{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glob은 해당하는 파일을 찾아줍니다. ../은 상위 폴더 주소 입니다. ./은 현재 폴더 주소입니다. \n",
    "\n",
    "실험 파일 전체에 적용되는 값들은 한 쪽 (주로 Jupyter notebook 맨 위쪽)에 몰아두는 것이 실수를 방지할 수 있습니다. (config.py를 만드는 것도 좋은 방법입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "RAW_DATA_FOLDER = './data/movie_reviews/'\n",
    "PREPROSSED_DATA_FOLDER = './tmp/'\n",
    "\n",
    "json_fnames = glob.glob('%s/*.json' % RAW_DATA_FOLDER)\n",
    "print(len(json_fnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(json_fnames[2], encoding='utf-8') as f:\n",
    "    json_article = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 문서는 네이버 영화의 리뷰를 수집한 것입니다. 이 중에서 우리가 분석에 이용할 텍스트 데이터만을 가지고 오겠습니다. \n",
    "\n",
    "코드는 항상 완벽할 수 없습니다. 데이터를 수집할 때 가능하다면 원본의 데이터를 늘 저장하시기 바랍니다. content를 만드는 parser가 잘못 만들어졌다면, 이미 저장된 JSON 파일들을 불러와서 다시 파싱하면 되니까요. \n",
    "\n",
    "영화 평점은 개봉 전 / 개봉 후 두 가지로 나뉘어져서 수집하였습니다. 각각의 리뷰는 몇 개씩 있는지 확인합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comments_after', 'comments_before'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_article.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 51)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_article['comments_before']), len(json_article['comments_after'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 comments_before, comments_after의 형식은 list of dict입니다. 하나만 꺼내서 구조를 살펴봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_article['comments_before'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_article['comments_before'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영화 평에 대한 agree, disagree, 평점, 평, 사용자아이디, 작성일시가 있습니다. 이 중에서 텍스트만을 가지고 와보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agree': 2,\n",
       " 'disagree': 0,\n",
       " 'score': 1,\n",
       " 'text': '...제목에서 덕필이 왔다...',\n",
       " 'user': '1441040',\n",
       " 'written_at': '2013.10.22 10:28'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_article['comments_before'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...제목에서 덕필이 왔다...'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_article['comments_before'][2]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나의 JSON에 들어있는 여러 개의 리뷰로부터 텍스트만을 가지고 오는 함수를 만들어 봅시다. for loop을 돌면서 text만을 꺼내서 리스트로 저장하면 됩니다. \n",
    "\n",
    "텍스트가 없거나 빈 텍스트일 경우 다음 for loop으로 넘어가기 위하여 if text: 를 넣었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for comment in json_article['comments_before']:\n",
    "    text = comment.get('text', '')\n",
    "    if not text:\n",
    "        continue\n",
    "    texts.append(text)\n",
    "    \n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개인적인 선호이지만, a line a document로 텍스트를 저장하는 편입니다. 나중에 term frequency matrix를 만들거나 Word2Vec, Doc2Vec을 학습할 때 편리합니다. \n",
    "\n",
    "하지만 하나의 문서는 여러 줄로 구성될 수 있습니다. 그래서 실제 줄바꿈이 있는 경우에는 **두 칸 띄어쓰기**를 이용하여 구분합니다. 기호에 따라서 탭 구분을 하셔도 좋습니다. 이 때, 줄 구분기호 (두 칸 띄어쓰기)는 반드시 한 문장 안에서 없어야 되겠죠. \n",
    "\n",
    "그리고 필요한 메타 정보들도 line number를 맞춰서 index 파일로 만드는 것을 선호합니다. 이렇게 line number가 맞으면 필요할 때 해당 문서의 메타 정보들을 이용할 수 있으니까요. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON 문서에 만약 text, written_at과 같은 정보가 존재하지 않아 오류가 날 것을 대비하여 dict.get(key, '')를 이용하여 json parser를 만듭니다. Python의 함수는 한 번에 여러 개의 변수가 return 될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def parse_text_from_json(json_fname):\n",
    "    with open(json_fname, encoding='utf-8') as f:        \n",
    "        json_object = json.load(f)\n",
    "        texts = []\n",
    "        for key in ['comments_before', 'comments_after']:\n",
    "            for comment in json_object.get(key, []):                \n",
    "                text = comment.get('text', '')\n",
    "                if not text:\n",
    "                    continue\n",
    "                texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "texts = parse_text_from_json(json_fnames[2])\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 파일을 tmp/test_reviews.txt로 저장해봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('%s/test_reviews.txt' % PREPROSSED_DATA_FOLDER, 'w', encoding='utf-8') as f:\n",
    "    for text in texts:\n",
    "        f.write('%s\\n' % text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평점과 텍스트를 탭으로 분리하여 하나의 파일에 함께 저장하고 싶다면 parser를 아래처럼 만들 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def parse_text_and_score_from_json(json_fname):\n",
    "    with open(json_fname, encoding='utf-8') as f:        \n",
    "        json_object = json.load(f)\n",
    "        texts = []\n",
    "        scores = []\n",
    "        for key in ['comments_before', 'comments_after']:\n",
    "            for comment in json_object.get(key, []):                \n",
    "                text = comment.get('text', '')\n",
    "                score = comment.get('score', -1)\n",
    "                if (not text) or (score == -1):\n",
    "                    continue\n",
    "                texts.append(text)\n",
    "                scores.append(score)\n",
    "        return texts, scores\n",
    "    \n",
    "texts, scores = parse_text_and_score_from_json(json_fnames[2])\n",
    "len(texts), len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip은 두 개 이상의 list (iterable한 set도 가능)를 함께 for loop을 돌 수 있도록 해줍니다. 만약 list의 길이가 다르다면 짧은 것에서 멈춥니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "무섭다.. 공포영화 1순위 ㄱㄱ싱! 9\n",
      "왠지 웃기기는 할듯. 일단 봐야 알겠음 9\n",
      "피판에서 봤는데, 역시 꽃과뱀 감독답게 아주 아름답고 좋았습니다.. 10\n",
      "애니메이션 설정을 영화로 표현하는건 좀 그러네요. 현실감이없어 일본은.... 1\n"
     ]
    }
   ],
   "source": [
    "for text, score in zip(texts[5:9], scores[5:]):\n",
    "    print(text, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 파일들을 돌면서 하나의 파일에 평점과 텍스트를 저장하겠습니다. \n",
    "\n",
    "    '%d\\t%s' % (score, text)\n",
    "    \n",
    "위 부분은 %d에 int를, %s에 str을 넣겠다는 의미입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9/t무섭다.. 공포영화 1순위 ㄱㄱ싱!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%d/t%s' % (scores[5], texts[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 영화 파일의 이름을 알아내기 위해서는 str.split()을 하면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/movie_reviews/100084.json\n",
      "100084.json\n",
      "100084\n"
     ]
    }
   ],
   "source": [
    "print(json_fnames[2])\n",
    "print(json_fnames[2].split('/')[-1])\n",
    "print(json_fnames[2].split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('%s/reviews.txt' % PREPROSSED_DATA_FOLDER, 'w', encoding='utf-8') as f:\n",
    "    for json_fname in json_fnames:\n",
    "        movie_idx = json_fname.split('/')[-1].split('.')[0]\n",
    "        texts, scores = parse_text_and_score_from_json(json_fname)\n",
    "        for score, text in zip(scores, texts):\n",
    "            f.write('%s\\t%d\\t%s\\n' % (movie_idx, score, text.replace('\\t', ' ').replace('\\n',' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
