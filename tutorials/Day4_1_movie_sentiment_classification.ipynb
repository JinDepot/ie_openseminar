{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토크나이징이 되어있는 데이터를 불러드립니다. 포멧을 확인합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72523\t명불허전\t1\n",
      "72523\t왠지 고사 피의 중간 고사 보다 재미 가 없을 듯해요 만약 보게 된다 면 실망 할듯\t1\n",
      "72523\t티아라 사랑 해 ㅜ\t10\n"
     ]
    }
   ],
   "source": [
    "movie_reviews = './data/merged_movie_comments_tsoynlp.txt'\n",
    "with open(movie_reviews, encoding='utf-8') as f:\n",
    "    for _ in range(3):\n",
    "        print(next(f).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "적절한 load_comments() 함수를 만들어 데이터를 로딩합니다. 3,280,685 개의 텍스트와 평점을 로딩하였습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3280685, 3280685)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_comments(fname):\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        docs = [doc.strip().split('\\t') for doc in f]\n",
    "    docs = [(doc[1], int(doc[2])) for doc in docs if len(doc) == 3]\n",
    "    texts, scores = zip(*docs)\n",
    "    return texts, scores\n",
    "\n",
    "texts, scores = load_comments(movie_reviews)\n",
    "len(texts), len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 min frequency cutting을 하기 위해 먼저 texts에 있는 모든 단어들의 빈도수를 확인합니다. \n",
    "\n",
    "    [word for text in texts for word in text.split()]\n",
    "    \n",
    "위 list 는 texts의 각 text마다 split()을 한 뒤, 그 단어들을 하나의 list로 flatten 하는 코드입니다. \n",
    "\n",
    "    ['a b', 'c d e'] --> ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "처럼 결과가 나옵니다. \n",
    "\n",
    "그런데, list가 아니라 맨 앞과 뒤를 [, ] 대신 (, )를 쓰면 generator가 만들어집니다. \n",
    "\n",
    "    (word for text in texts for word in text.split())\n",
    "    \n",
    "generator는 한마디로 말하면 (정확한 표현이 아닙니다), 메모리에 필요한 값만 올려서 쓰는 list라고 생각하셔도 됩니다. 이미 있는 texts라는 list를 flatten한 새로운 list는 만들지 않고 쓰겠다는 의미입니다. \n",
    "\n",
    "여러분은 이미 generator를 쓰시고 계십니다. open(fname) 역시 한 줄을 읽고 이를 yield 하는 generator입니다. \n",
    "\n",
    "또한 psutil이라는 package를 쓰면 현재 프로세스 (주피터 파일, 혹은 파이썬 파일)이 사용하는 메모리의 양이 출력됩니다. soynlp.utils에 get_process_memory() 라는 함수로 wrapping 해두었습니다. 필요하실 때 쓰십시요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used memory = 1.346 Gb\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counter = Counter((word for text in texts for word in text.split()))\n",
    "\n",
    "from soynlp.utils import get_process_memory\n",
    "print('used memory = %.3f Gb' % get_process_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빈도수 기준 상위 20개의 단어를 출력합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영화', 1412516),\n",
       " ('이', 764006),\n",
       " ('관람객', 585858),\n",
       " ('는', 459876),\n",
       " ('가', 438771),\n",
       " ('도', 418073),\n",
       " ('의', 403943),\n",
       " ('다', 381746),\n",
       " ('재밌', 370724),\n",
       " ('재미', 344634),\n",
       " ('너무', 335529),\n",
       " ('ㅋㅋ', 321284),\n",
       " ('정말', 297962),\n",
       " ('고', 294644),\n",
       " ('을', 270826),\n",
       " ('에', 266720),\n",
       " ('한', 263385),\n",
       " ('를', 263311),\n",
       " ('연기', 255673),\n",
       " ('최고', 254291)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_counter.items(), key=lambda x:x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_count = 50으로 frequency cutting 을 할 때, 단어의 개수의 차이를 확인합니다. 총 22,451개의 단어가 존재합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341744 --> 22451\n"
     ]
    }
   ],
   "source": [
    "n_words_before_pruning = len(word_counter)\n",
    "\n",
    "min_count = 50\n",
    "word_dictionary = {word:freq for word,freq in word_counter.items() if freq >= min_count}\n",
    "n_words_after_pruning  = len(word_dictionary)\n",
    "\n",
    "print('%d --> %d' % (n_words_before_pruning, n_words_after_pruning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평점 별로도 문서의 개수를 확인합니다. 유명한 영화들이다보니 10점이 압도적으로 많음을 알 수 있습니다. 애초에 좋은 영화가 아니면 사람들이 많이 보지 않았겠죠? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 1: (320898, 9.781 perc)\n",
      "score = 2: (34351, 1.047 perc)\n",
      "score = 3: (35580, 1.085 perc)\n",
      "score = 4: (39742, 1.211 perc)\n",
      "score = 5: (78250, 2.385 perc)\n",
      "score = 6: (95834, 2.921 perc)\n",
      "score = 7: (149618, 4.561 perc)\n",
      "score = 8: (268622, 8.188 perc)\n",
      "score = 9: (344905, 10.513 perc)\n",
      "score = 10: (1912885, 58.307 perc)\n"
     ]
    }
   ],
   "source": [
    "for score, score_freq in Counter(scores).items():\n",
    "    print('score = %d: (%d, %.3f perc)' % (score, score_freq, 100*score_freq/len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min count cutting을 하다보니 희귀한 단어로만 이뤄진 문장도 존재할 수 있습니다. 이 경우에는 zero vector가 만들어질겁니다. 이를 방지하기 위해서 학습용 데이터를 따로 만들겠습니다. \n",
    "\n",
    "그리고 binary classification을 하기 위해서 3점 이하의 영화평을 negative class, 9점 이상의 영화평을 positive class로 만들겠습니다. \n",
    "\n",
    "    words = [word for word in text.split() if word in word_dictionary]\n",
    "    if not words:\n",
    "        continue\n",
    "        \n",
    "위 부분은 text를 split() 한 다음, 각 단어가 word_dictionary (min_count >= 50인 단어 집합)에 등록되었는지 확인합니다. 만약 words가 empty list이면 학습데이터에 추가하지 않고 다음 text를 살펴봅니다. \n",
    "\n",
    "    train_label.append(1 if score > 8 else -1)\n",
    "    \n",
    "위 부분은 평점이 9 혹은 10점이면 1이라는 label을, 그렇지 않다면 -1이라는 label을 train_label에 추가합니다. 어자피 [4, 8]점의 영화평들은 아래의 구문에 의하여 존재하지 않으니까요. \n",
    "\n",
    "    if 4 <= score <= 8:\n",
    "        continue\n",
    "        \n",
    "여기서 한 가지 조금 특이하게 train_texts를 만들었습니다. words는 list of str입니다. 그렇기 때문에 train_texts는 list of list of str입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 3280685 --> 2641694\n",
      "label = 1: (2252779, 85.278 perc)\n",
      "label = -1: (388915, 14.722 perc)\n"
     ]
    }
   ],
   "source": [
    "train_texts = []\n",
    "train_label = []\n",
    "\n",
    "for text, score in zip(texts, scores):\n",
    "    if 4 <= score <= 8:\n",
    "        continue\n",
    "        \n",
    "    words = [word for word in text.split() if word in word_dictionary]\n",
    "    if not words:\n",
    "        continue\n",
    "    \n",
    "    train_texts.append(words)\n",
    "    train_label.append(1 if score > 8 else -1)\n",
    "\n",
    "print('train data: %d --> %d' % (len(texts), len(train_texts)))\n",
    "for label, label_freq in Counter(train_label).items():\n",
    "    print('label = %d: (%d, %.3f perc)' % (label, label_freq, 100*label_freq/len(train_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer를 이용하여 train_x, term frequency matrix를 만들겠습니다. 이미 train_texts는 토크나이징이 완료되어 있습니다. CountVectorizer에서 lowercase=False로 만들고, tokenizer를 lambda x:x로 두면 아무런 처리를 하지 않은 체 train_texts를 입력할 수 있습니다. \n",
    "\n",
    "sparse matrix에서 이용한 단어의 개수가 22,451개로 word_dictionary의 개수와 같습니다. min_df, max_df를 설정하지 않았기 때문에 빈도수가 50이상인 모든 단어들을 이용하여 sparse matrix를 만들었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2641694, 22451)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x:x, lowercase=False)\n",
    "train_x = vectorizer.fit_transform(train_texts)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 실습처럼 vectorizer.vocabulary\\_로부터 vocablist를 만들 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('vocablist', 'w', encoding='utf-8') as f:\n",
    "#     for word, _ in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1]):\n",
    "#         f.write('%s\\n' % word)\n",
    "vocablist = [word for word, _ in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1주일', '1차', '1초', '1초도', '1탄']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocablist[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지하여 만들어둔 데이터를 data/ 폴더에 저장해 두었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "if SAVE:\n",
    "    import pickle\n",
    "    with open('./tmp/sentiment_x.pkl', 'wb') as f:\n",
    "        pickle.dump(train_x, f)\n",
    "\n",
    "    with open('./tmp/sentiment_y.pkl', 'wb') as f:\n",
    "        pickle.dump(train_label, f)\n",
    "        \n",
    "    with open('./tmp/sentiment_vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(vocablist, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularization이 Logistic Regression의 기본모델입니다. 물론 L2 cost를 C = 0으로 주면 전혀 regularization이 일어나지 않습니다만, 많은 경우 overfitting이 일어나니 L2를 이용하시기 바랍니다. default cost는 C=1입니다. \n",
    "\n",
    "    logistic_l2 = LogisticRegression(C = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_l2 = LogisticRegression()\n",
    "logistic_l2.fit(train_x, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의의 문장 30, 50, 64323에 대하여 실제 데이터와 레이블, 그리고 logistic regression이 예측하는 class probability / class label을 출력합니다. \n",
    "\n",
    "    logistic_l2.predict_proba(text_x)\n",
    "    \n",
    "Logistic Regression은 predict 의 input으로 matrix가 들어올 것을, 동시에 여러 개의 queries가 들어올 것을 가정하고 만든 알고리즘입니다. 하나의 query를 prediction 할 때에는 그 결과값을\n",
    "\n",
    "    logistic_l2.predict_proba(train_x[idx,:])[0]\n",
    "    \n",
    "과 같이 하여야 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['지연', '누나', '보러', '왓는데']\n",
      "label: 1\n",
      "\n",
      "class prob: (negative= 0.142, positive= 0.858\n",
      "class = positive\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "text: ['솔직', '희', '진짜', '별로', '재미', '없음', '지연', '누나', '땜시', '1개', '반', '준거', '임']\n",
      "label: -1\n",
      "\n",
      "class prob: (negative= 0.985, positive= 0.015\n",
      "negative\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "text: ['진짜', '엄청', '나다', '이', '영화', '를', '볼수있', '다니', '행복', '지금', '12', '살', '미만', '인', '아이', '들이', '불쌍', '하다', '이런', '영화', '를', '영화', '관', '에서', '못보', '다니', '진짜', '엄청', '난', '영화', '다', '재밌', '냐고', '재미', '정도', '가', '아니', '다']\n",
      "label: 1\n",
      "\n",
      "class prob: (negative= 0.001, positive= 0.999\n",
      "class = positive\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in [30, 50, 64323]:\n",
    "    print('text: %s\\nlabel: %d\\n' % (train_texts[idx], train_label[idx]))\n",
    "    \n",
    "    prob = logistic_l2.predict_proba(train_x[idx,:])[0]\n",
    "    print('class prob: (negative= %.3f, positive= %.3f' \n",
    "          % tuple(prob))\n",
    "    \n",
    "    pred_label = logistic_l2.predict(train_x[idx,:])[0]\n",
    "    print('class = %s' % 'positive' if pred_label == 1 else 'negative')\n",
    "    print('\\n%s\\n' % ('-'*50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso penalty를 Logistic Regression에 넣어 보겠습니다. Regularization을 강하게 주기 위하여 C = 0.1로 주었습니다. 학습하는 부분은 동일합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_l1 = LogisticRegression(C=0.1, penalty='l1')\n",
    "logistic_l1.fit(train_x, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 동일한 test를 해보았습니다. 결과는 비슷합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['지연', '누나', '보러', '왓는데']\n",
      "label: 1\n",
      "\n",
      "class prob: (negative= 0.140, positive= 0.860\n",
      "class = positive\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "text: ['솔직', '희', '진짜', '별로', '재미', '없음', '지연', '누나', '땜시', '1개', '반', '준거', '임']\n",
      "label: -1\n",
      "\n",
      "class prob: (negative= 0.980, positive= 0.020\n",
      "negative\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "text: ['진짜', '엄청', '나다', '이', '영화', '를', '볼수있', '다니', '행복', '지금', '12', '살', '미만', '인', '아이', '들이', '불쌍', '하다', '이런', '영화', '를', '영화', '관', '에서', '못보', '다니', '진짜', '엄청', '난', '영화', '다', '재밌', '냐고', '재미', '정도', '가', '아니', '다']\n",
      "label: 1\n",
      "\n",
      "class prob: (negative= 0.001, positive= 0.999\n",
      "class = positive\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in [30, 50, 64323]:\n",
    "    print('text: %s\\nlabel: %d\\n' % (train_texts[idx], train_label[idx]))\n",
    "    \n",
    "    prob = logistic_l1.predict_proba(train_x[idx,:])[0]\n",
    "    print('class prob: (negative= %.3f, positive= %.3f' \n",
    "          % tuple(prob))\n",
    "    \n",
    "    pred_label = logistic_l1.predict(train_x[idx,:])[0]\n",
    "    print('class = %s' % 'positive' if pred_label == 1 else 'negative')\n",
    "    print('\\n%s\\n' % ('-'*50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression의 Coefficients를 뜯어보겠습니다. \n",
    "\n",
    "    LogisticRegression.coef_\n",
    "    \n",
    "는 (1 by n_vocab)의 ndarray입니다. 우리가 binary classification을 하였기 때문입니다. multi class logistic regression을 하면 (n_class by n_vocab)의 coefficient matrix가 만들어집니다. \n",
    "\n",
    "학습하는 방법은 위와 동일합니다. LogisticRegression이 train_label의 unique label의 개수를 확인한 뒤, multi-class classification이면 알아서 Softmax regression으로 바꿔둡니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 22451)\n"
     ]
    }
   ],
   "source": [
    "print(type(logistic_l1.coef_))\n",
    "print(logistic_l1.coef_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 ndarray를 list로 풀어보겠습니다. (1 by n_vocab)이기 때문에 coefficients[0]을 하면 positive class를 예측하는 각 단어의 coefficient가 출력됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.29573326401702005,\n",
       " 0.06048288324875791,\n",
       " 0.3599387791174224,\n",
       " 0.06875783921374953,\n",
       " -0.055198722191172445]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = logistic_l1.coef_.tolist()\n",
    "coefficients[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive class에 가까운 단어들을 살펴보겠습니다. coefficient의 enumerate를 sorting하면 각 단어의 coefficient가 큰 순서대로 정렬되어 출력됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20531, 3.6810552915713934),\n",
       " (3702, 3.173158488667234),\n",
       " (6710, 2.7119258656919767),\n",
       " (2762, 2.702309178468442),\n",
       " (17070, 2.696817974616877)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_coefficients = sorted(enumerate(coefficients[0]), key=lambda x:x[1], reverse=True)\n",
    "sorted_coefficients[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "긍정적인 영화평에서 자주 나오는 단어들 상위 50개를 출력하면 아래와 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "틈이 (3.681)\n",
      "꿀잼 (3.173)\n",
      "또보고싶 (2.712)\n",
      "굿 (2.702)\n",
      "재밋어요 (2.697)\n",
      "시키지 (2.678)\n",
      "쵝오 (2.584)\n",
      "굳 (2.538)\n",
      "여운이 (2.497)\n",
      "존잼 (2.494)\n",
      "최고 (2.468)\n",
      "짱임 (2.468)\n",
      "흠잡을 (2.436)\n",
      "알이즈웰 (2.429)\n",
      "완벽 (2.343)\n",
      "굿굿 (2.341)\n",
      "대박 (2.311)\n",
      "짱 (2.279)\n",
      "잼씀 (2.245)\n",
      "잊을수 (2.228)\n",
      "있구만 (2.223)\n",
      "시간가는줄 (2.217)\n",
      "졸잼 (2.201)\n",
      "안아까운 (2.199)\n",
      "안아까 (2.197)\n",
      "최곱니다 (2.169)\n",
      "Good (2.165)\n",
      "꼭보 (2.123)\n",
      "짱이 (2.106)\n",
      "재밋엇어요 (2.097)\n",
      "안보면 (2.041)\n",
      "강추 (2.020)\n",
      "따뜻 (2.004)\n",
      "뗄수 (2.003)\n",
      "이정도면 (1.987)\n",
      "굳굳 (1.979)\n",
      "은위 (1.959)\n",
      "짱짱 (1.944)\n",
      "먹먹 (1.943)\n",
      "있던데 (1.941)\n",
      "완벽한 (1.937)\n",
      "잼나 (1.937)\n",
      "만으로도 (1.934)\n",
      "만족 (1.925)\n",
      "슬퍼요 (1.913)\n",
      "아깝지 (1.909)\n",
      "점줌 (1.906)\n",
      "낮지 (1.905)\n",
      "할틈 (1.903)\n",
      "좋네요 (1.880)\n"
     ]
    }
   ],
   "source": [
    "for word, coef in sorted_coefficients[:50]:\n",
    "    print('%s (%.3f)' % (vocablist[word], coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부정적인 영화평에서 자주 나오는 단어들 상위 50개를 출력하면 아래와 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O기 (-2.135)\n",
      "시청자 (-2.138)\n",
      "총기를 (-2.154)\n",
      "거품이 (-2.155)\n",
      "컸나 (-2.155)\n",
      "이하도 (-2.177)\n",
      "딴걸 (-2.194)\n",
      "졸았 (-2.194)\n",
      "비추 (-2.197)\n",
      "이하 (-2.201)\n",
      "숙면 (-2.202)\n",
      "싶지 (-2.238)\n",
      "아까 (-2.242)\n",
      "졸려 (-2.251)\n",
      "그다지 (-2.254)\n",
      "할인카드 (-2.257)\n",
      "망작 (-2.308)\n",
      "안볼란다 (-2.312)\n",
      "잤어요 (-2.313)\n",
      "음모론 (-2.323)\n",
      "이딴 (-2.332)\n",
      "그닥 (-2.351)\n",
      "나가고 (-2.352)\n",
      "퇴보 (-2.352)\n",
      "잤다 (-2.354)\n",
      "김일병 (-2.374)\n",
      "엉망 (-2.385)\n",
      "실망 (-2.390)\n",
      "점준 (-2.408)\n",
      "빵점 (-2.421)\n",
      "거품 (-2.427)\n",
      "웃대 (-2.448)\n",
      "이딴게 (-2.508)\n",
      "ㅁㅈㅎ (-2.558)\n",
      "짜집기 (-2.569)\n",
      "차라리 (-2.572)\n",
      "긴급조치 (-2.620)\n",
      "별루 (-2.623)\n",
      "불면증 (-2.670)\n",
      "긴급조치19호 (-2.770)\n",
      "졸작 (-2.790)\n",
      "과대평가 (-2.965)\n",
      "방어율 (-3.088)\n",
      "내돈 (-3.097)\n",
      "핵노잼 (-3.166)\n",
      "노잼 (-3.224)\n",
      "최악이다 (-3.337)\n",
      "돈아까 (-3.412)\n",
      "최악의 (-4.026)\n",
      "최악 (-4.100)\n"
     ]
    }
   ],
   "source": [
    "for word, coef in sorted_coefficients[-50:]:\n",
    "    print('%s (%.3f)' % (vocablist[word], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
